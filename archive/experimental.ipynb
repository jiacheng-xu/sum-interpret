{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, description='Downloading', max=1099, style=ProgressStyle(description_width…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bf65d0dfb23441a90be2f45820f0214"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, description='Downloading', max=2275329241, style=ProgressStyle(description…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fa70a1d874049e884d427b4a10462c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-8be859e46dc2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPegasusTokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPegasusForConditionalGeneration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPegasusForConditionalGeneration\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"google/pegasus-xsum\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPegasusTokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"google/pegasus-xsum\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/modeling_utils.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    830\u001B[0m                     \u001B[0mproxies\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    831\u001B[0m                     \u001B[0mresume_download\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresume_download\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 832\u001B[0;31m                     \u001B[0mlocal_files_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlocal_files_only\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    833\u001B[0m                 )\n\u001B[1;32m    834\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mresolved_archive_file\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001B[0m\n\u001B[1;32m    688\u001B[0m             \u001B[0mresume_download\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresume_download\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    689\u001B[0m             \u001B[0muser_agent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muser_agent\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 690\u001B[0;31m             \u001B[0mlocal_files_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlocal_files_only\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    691\u001B[0m         )\n\u001B[1;32m    692\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl_or_filename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001B[0m\n\u001B[1;32m    867\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%s not found in cache or force_download set to True, downloading to %s\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_file\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 869\u001B[0;31m             \u001B[0mhttp_get\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproxies\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresume_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresume_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muser_agent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muser_agent\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    870\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    871\u001B[0m         \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"storing %s in cache at %s\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcache_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mhttp_get\u001B[0;34m(url, temp_file, proxies, resume_size, user_agent)\u001B[0m\n\u001B[1;32m    760\u001B[0m         \u001B[0mdisable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_verbosity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlogging\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNOTSET\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    761\u001B[0m     )\n\u001B[0;32m--> 762\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mchunk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miter_content\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1024\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mchunk\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# filter out keep-alive new chunks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m             \u001B[0mprogress\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/requests/models.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    748\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'stream'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 750\u001B[0;31m                     \u001B[0;32mfor\u001B[0m \u001B[0mchunk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraw\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecode_content\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    751\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mchunk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mProtocolError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/response.py\u001B[0m in \u001B[0;36mstream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m    492\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    493\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_fp_closed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 494\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecode_content\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdecode_content\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    495\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    496\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/response.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    440\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m                 \u001B[0mcache_content\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 442\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    443\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mamt\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Platform-specific: Buggy versions of Python.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    444\u001B[0m                     \u001B[0;31m# Close the connection when no data is returned\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/http/client.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    447\u001B[0m             \u001B[0;31m# Amount is given, implement using readinto\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m             \u001B[0mb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbytearray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 449\u001B[0;31m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    450\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mmemoryview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    451\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/http/client.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    491\u001B[0m         \u001B[0;31m# connection, and the user is reading more bytes than will be provided\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m         \u001B[0;31m# (for example, reading in 1k chunks)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 493\u001B[0;31m         \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    494\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mn\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    495\u001B[0m             \u001B[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    584\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py\u001B[0m in \u001B[0;36mrecv_into\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    292\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    293\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 294\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    295\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mOpenSSL\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSSL\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSysCallError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    296\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msuppress_ragged_eofs\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Unexpected EOF'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/OpenSSL/SSL.py\u001B[0m in \u001B[0;36mrecv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1819\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSSL_peek\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ssl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnbytes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1820\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1821\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSSL_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ssl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnbytes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1822\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_raise_ssl_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ssl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1823\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from captum.attr import InputXGradient,DeepLiftShap\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "PGE_ARTICLE = \"PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    "mname = \"google/pegasus-xsum\"\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_texts=[PGE_ARTICLE])  # don't need tgt_text for inference\n",
    "gen = model.generate(**batch)  # for forward pass: model(**batch)\n",
    "summary: List[str] = tokenizer.batch_decode(gen, skip_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BART"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-xsum')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "encoder = model.get_encoder()\n",
    "\n",
    "TXT = \"My friends are good but they eat too many .\"\n",
    "input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']\n",
    "encoder_outputs = encoder(input_ids, return_dict=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def wrap_mask_filler(inp):\n",
    "    # assume batch size = 1\n",
    "    print(inp)\n",
    "    print(inp.size())\n",
    "    scores = model(inp)[0]\n",
    "    masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "    probs = scores[0, masked_index].softmax(dim=-1)\n",
    "    return probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)\n",
    "\n",
    "lig = LayerIntegratedGradients(wrap_mask_filler, model.model.shared)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>My', 'friends', 'are', 'good', 'but', 'they', 'eat', 'too', 'many<mask>.</s>']\n",
      "tensor([[    0,  2387,   964,    32,   205,    53,    51,  3529,   350,   171,\n",
      "         50264,   479,     2]])\n",
      "torch.Size([1, 13])\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]])\n",
      "torch.Size([1, 13])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TXT = \"My friends are good but they eat too many <mask> .\"\n",
    "input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']\n",
    "print(tokenizer.decode(input_ids[0]).split())\n",
    "seq_length = input_ids.shape[1]\n",
    "reference_indices = token_reference.generate_reference(seq_length, device='cpu').unsqueeze(0)\n",
    "reference_indices[:,0] = tokenizer.bos_token_id\n",
    "reference_indices[:,-1] = tokenizer.eos_token_id\n",
    "\n",
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "attributions_ig, delta = lig.attribute(input_ids, baselines=reference_indices, \\\n",
    "                                           n_steps=50, return_convergence_delta=True)\n",
    "\n",
    "# probs = outputs[0, masked_index].softmax(dim=-1)\n",
    "# values, predictions = probs.topk(5)\n",
    "# tokenizer.decode(predictions).split()\n",
    "# ['good', 'great', 'all', 'really', 'very']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "def load_BART(mname='facebook/bart-large-xsum'):\n",
    "\n",
    "    # Mask filling only works for bart-large\n",
    "    from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "    tokenizer = BartTokenizer.from_pretrained(mname)\n",
    "    model = BartForConditionalGeneration.from_pretrained(mname)\n",
    "    return model, tokenizer\n",
    "model, tokenizer = load_BART()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "from transformers import BatchEncoding, PreTrainedTokenizer\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients, LayerGradientShap, LayerGradientXActivation, TokenReferenceBase\n",
    "\n",
    "\n",
    "def read_json_data(fdir, fname):\n",
    "    \"\"\"Read json file from fdir/fname. Assume it contains key 'data'.\"\"\"\n",
    "    fp = open(os.path.join(fdir, fname), 'r')\n",
    "    data_dict = json.load(fp)['data']\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def get_input_docs_from_json(data_dict: List[Dict], use_add_sent=True):\n",
    "    outputs = []\n",
    "    outputs_qa_pairs = []\n",
    "    for data in data_dict:\n",
    "        inp_doc, add_sent = data['input_doc'], data['added_sent']\n",
    "        input_str = inp_doc if not use_add_sent else \"{} {}\".format(\n",
    "            inp_doc, add_sent)\n",
    "        outputs.append(input_str)\n",
    "\n",
    "        mask_pairs = data['mask_pairs']\n",
    "        for mask_pair in mask_pairs:\n",
    "            q, a, wa = mask_pair['q'], mask_pair['a'], mask_pair['wa']\n",
    "            outputs_qa_pairs.append(\n",
    "                {\n",
    "                    'context': input_str,\n",
    "                    'q': q,\n",
    "                    'a': a,\n",
    "                    'wa': wa\n",
    "                }\n",
    "            )\n",
    "    return outputs, outputs_qa_pairs\n",
    "\n",
    "\n",
    "import torch\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "\n",
    "def attr_visualization():\n",
    "    pass\n",
    "\n",
    "\n",
    "# layer wise: inputs, baselines, target, additional_forward_args\n",
    "# for each summary, the model needs to encode the document again and again?\n",
    "\n",
    "class SumGen(torch.nn.Module):\n",
    "    def __init__(self, model, tokenizer: PreTrainedTokenizer, attribution_func, max_len=50):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.use_cache = True\n",
    "        self.attr = attribution_func(self.forward_step, self.model.model.shared)\n",
    "        self.encoder = self.model.get_encoder()\n",
    "\n",
    "    def prepare_batch_inp(self, input_articles: List[str], tgt_summaries: Optional[List[str]]) -> BatchEncoding:\n",
    "        return self.tokenizer.prepare_seq2seq_batch(src_texts=input_articles, tgt_texts=tgt_summaries,\n",
    "                                                    return_tensors='pt')\n",
    "\n",
    "    def run_attribution(self, input_doc, attn_mask, tgt_sum=None):\n",
    "\n",
    "        device = input_doc.device\n",
    "        batch_size = input_doc.shape[0]\n",
    "        encoder = self.model.get_encoder()\n",
    "        cur_len = 1\n",
    "        has_eos = [False for _ in range(batch_size)]\n",
    "        bos_token_id = self.tokenizer.bos_token_id\n",
    "        decoded = [[bos_token_id] for _ in range(batch_size)]\n",
    "        decoder_input_ids = torch.LongTensor(decoded).to(device)\n",
    "        past_key_values = None\n",
    "        seq_length = input_doc.shape[1]\n",
    "        token_reference = TokenReferenceBase(reference_token_idx=self.tokenizer.pad_token_id)\n",
    "        reference_indice = token_reference.generate_reference(seq_length, device=device)\n",
    "        reference_indices = torch.stack([reference_indice for _ in range(batch_size)], dim=0)\n",
    "        reference_indices[:, 0] = self.tokenizer.bos_token_id\n",
    "        reference_indices[:, -1] = self.tokenizer.eos_token_id\n",
    "\n",
    "        while cur_len < self.max_len and (not all(has_eos)):\n",
    "            additional_input = {\n",
    "                \"attn_mask\": attn_mask,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"decoder_input_ids\": decoder_input_ids, \"attr_mode\": False\n",
    "            }\n",
    "            cur_decoded, cur_past_key_values, cur_decoder_input_ids = self.forward_step(input_doc,additional_input\n",
    "                                                                                        )\n",
    "            # cur_decoded is just a list with token id\n",
    "            for idx, cur_dec_tok in enumerate(cur_decoded):\n",
    "                if cur_dec_tok == self.tokenizer.eos_token_id:\n",
    "                    has_eos[idx] = True\n",
    "            if tgt_sum is None:\n",
    "                target = cur_decoder_input_ids[:, -1].unsqueeze(0)\n",
    "                target = cur_decoded\n",
    "            else:\n",
    "                pass\n",
    "            additional_input = {\n",
    "                \"attn_mask\": attn_mask,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"decoder_input_ids\": decoder_input_ids, \"attr_mode\": True\n",
    "            }\n",
    "            attribution, delta = self.attr.attribute(inputs=input_doc, baselines=reference_indices, target=target,\n",
    "                                                     additional_forward_args=additional_input\n",
    "                                                     )\n",
    "            past_key_values = cur_past_key_values\n",
    "            decoder_input_ids = cur_decoder_input_ids\n",
    "        print(\"end of decoding\")\n",
    "\n",
    "    def forward_step(self, input_doc,\n",
    "                     additional_input_args: dict,\n",
    "                     # attn_mask, past_key_values, decoder_input_ids, attr_mode: bool\n",
    "                     ):\n",
    "        attn_mask, past_key_values, decoder_input_ids, attr_mode = \\\n",
    "            additional_input_args['attn_mask'], additional_input_args['past_key_values'], additional_input_args[\n",
    "                'decoder_input_ids'], additional_input_args['attr_mode'],\n",
    "        encoder_outputs = self.encoder(input_doc, attention_mask=attn_mask, return_dict=True)\n",
    "        batch_size = input_doc.shape[0]\n",
    "        device = input_doc.device\n",
    "\n",
    "        expanded_batch_idxs = (\n",
    "            torch.arange(batch_size)\n",
    "                .view(-1, 1)\n",
    "                .repeat(1, 1)\n",
    "                .view(-1)\n",
    "                .to(device)\n",
    "        )\n",
    "        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "            0, expanded_batch_idxs\n",
    "        )\n",
    "\n",
    "        model_inputs = {\"input_ids\": None,\n",
    "                        \"past_key_values\": past_key_values,\n",
    "                        \"attention_mask\": attn_mask,\n",
    "                        \"encoder_outputs\": encoder_outputs,\n",
    "                        \"decoder_input_ids\": decoder_input_ids,\n",
    "                        }\n",
    "\n",
    "        outputs = self.model(**model_inputs, use_cache=True, return_dict=True)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "        if attr_mode:\n",
    "            return next_token_logits.unsqueeze(0)\n",
    "        next_token = next_token.unsqueeze(-1)\n",
    "        cur_decoded = next_token.tolist()\n",
    "        if \"past_key_values\" in outputs:\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "        return cur_decoded, past_key_values, decoder_input_ids\n",
    "\n",
    "    def forward_entrance(self, input_doc, attention_mask, tgt_sum):\n",
    "\n",
    "        device = input_doc.device\n",
    "        batch_size = input_doc.shape[0]\n",
    "        encoder = self.model.get_encoder()\n",
    "        encoder_outputs = encoder(input_doc, attention_mask=attention_mask, return_dict=True)\n",
    "        cur_len = 1\n",
    "        decoded = [[self.tokenizer.bos_token_id] for _ in range(batch_size)]\n",
    "        decoder_input_ids = torch.LongTensor(decoded).to(device)\n",
    "        next_token = decoder_input_ids\n",
    "        past = None\n",
    "        expanded_batch_idxs = (\n",
    "            torch.arange(batch_size)\n",
    "                .view(-1, 1)\n",
    "                .repeat(1, 1)\n",
    "                .view(-1)\n",
    "                .to(device)\n",
    "        )\n",
    "        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "            0, expanded_batch_idxs\n",
    "        )\n",
    "        all_logits = []\n",
    "        while cur_len < self.max_len:\n",
    "            self.forward_step()\n",
    "            attributions_ig, delta = self.attr.attribute(input_ids, baselines=reference_indices, \\\n",
    "                                                         n_steps=50, return_convergence_delta=True)\n",
    "\n",
    "    def forward(self, input_doc, input_doc_attn_mask, tgt_sum=None):\n",
    "        device = input_doc.device\n",
    "        batch_size = input_doc.shape[0]\n",
    "        encoder = self.model.get_encoder()\n",
    "        encoder_outputs = encoder(input_doc, attention_mask=input_doc_attn_mask, return_dict=True)\n",
    "        cur_len = 1\n",
    "        decoded = [[self.tokenizer.bos_token_id] for _ in range(batch_size)]\n",
    "        decoder_input_ids = torch.LongTensor(decoded).to(device)\n",
    "        next_token = decoder_input_ids\n",
    "        past = None\n",
    "        expanded_batch_idxs = (\n",
    "            torch.arange(batch_size)\n",
    "                .view(-1, 1)\n",
    "                .repeat(1, 1)\n",
    "                .view(-1)\n",
    "                .to(device)\n",
    "        )\n",
    "        encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "            0, expanded_batch_idxs\n",
    "        )\n",
    "        all_logits = []\n",
    "        while cur_len < self.max_len:\n",
    "            model_inputs = {\"input_ids\": None,\n",
    "                            \"past_key_values\": past,\n",
    "                            \"attention_mask\": input_doc_attn_mask,\n",
    "                            \"encoder_outputs\": encoder_outputs,\n",
    "                            \"decoder_input_ids\": decoder_input_ids,\n",
    "                            }\n",
    "\n",
    "            outputs = self.model(**model_inputs, use_cache=True, return_dict=True)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            all_logits.append(next_token_logits)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "            next_token = next_token.unsqueeze(-1)\n",
    "            cur_decoded = next_token.tolist()\n",
    "            decoded = [already_dec + cur_decoded[idx] for idx, already_dec in enumerate(decoded)]\n",
    "            if \"past_key_values\" in outputs:\n",
    "                past = outputs.past_key_values\n",
    "\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "            cur_len += 1\n",
    "        # while cur_len < self.max_len:\n",
    "        #     model_inputs = {\"input_ids\": decoder_input_ids,\n",
    "        #                     \"past_key_values\": past,\n",
    "        #                     \"attention_mask\": input_doc_attn_mask,\n",
    "        #                     \"encoder_outputs\": encoder_outputs,\n",
    "        #                     # \"decoder_input_ids\": next_token,\n",
    "        #                     \"use_cache\": self.use_cache}\n",
    "        #\n",
    "        #     outputs = self.model(**model_inputs, return_dict=True)\n",
    "        #     next_token_logits = outputs.logits[:, -1, :]\n",
    "        #     next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "        #     next_token = next_token.unsqueeze(-1)\n",
    "        #     cur_decoded = next_token.tolist()\n",
    "        #     decoded = [ already_dec+cur_decoded[idx] for idx,already_dec in enumerate(decoded)]\n",
    "        #     if \"past_key_values\" in outputs:\n",
    "        #         past = outputs.past_key_values\n",
    "        #\n",
    "        #     decoder_input_ids = torch.cat([decoder_input_ids, next_token],dim=-1)\n",
    "        #     cur_len += 1\n",
    "        for dec in decoded:\n",
    "            print(self.tokenizer.decode(dec).split())\n",
    "\n",
    "        torch.stack(all_logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SumGen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-cfa068b4da01>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0msgen\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSumGen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mattribution_func\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mLayerIntegratedGradients\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m example_input = [\"The Pegasus model was proposed in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019. According to the abstract, Pegasus’ pretraining task is intentionally similar to summarization: important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\",\n\u001B[1;32m      5\u001B[0m              \"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer’s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.\"]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'SumGen' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sgen = SumGen(model=model,tokenizer=tokenizer,attribution_func=LayerIntegratedGradients)\n",
    "example_input = [\"The Pegasus model was proposed in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019. According to the abstract, Pegasus’ pretraining task is intentionally similar to summarization: important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\",\n",
    "             \"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer’s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.\"]\n",
    "example_output = [\"The paper is accepted by ICML.\", \"The paper comes from AI2.\"]\n",
    "\n",
    "example_input = example_input[:1]\n",
    "example_output = example_output[:1]\n",
    "data_w_label  = sgen.prepare_batch_inp(example_input, example_output)   # data_w_label['data']['input_ids']  and data_w_label['data']['labels']\n",
    "\n",
    "# sgen.run_attribution(data_w_label.data['input_ids'], data_w_label.data['attention_mask'])\n",
    "sgen.run_attribution(data_w_label.data['input_ids'], None)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}