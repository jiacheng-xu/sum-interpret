{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.3.1 available.\n",
      "TensorFlow version 2.3.0 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-xsum')\n",
    "encoder = model.get_encoder()\n",
    "\n",
    "\n",
    "TXT =\"The Pegasus model was proposed in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019. According to the abstract, Pegasus’ pretraining task is intentionally similar to summarization: important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\"\n",
    "input_ids = tokenizer([TXT]*23, return_tensors='pt')['input_ids']\n",
    "out = encoder(input_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# BART function\n",
    "def forward_step(input_doc,decoder_input_ids,\n",
    "                 additional_input_args: dict,\n",
    "                 # attn_mask, past_key_values, decoder_input_ids, attr_mode: bool\n",
    "                 ):\n",
    "    attn_mask, past_key_values, _, attr_mode = \\\n",
    "        additional_input_args['attn_mask'], additional_input_args['past_key_values'], additional_input_args[\n",
    "            'decoder_input_ids'], additional_input_args['attr_mode']\n",
    "    print(input_doc.size())\n",
    "    print(decoder_input_ids.size())\n",
    "    # input_doc = input_doc.permute(1,0)\n",
    "    # decoder_input_ids = decoder_input_ids.permute(1,0)\n",
    "    encoder_outputs = encoder(input_doc, attention_mask=None, return_dict=True)\n",
    "    batch_size = input_doc.shape[0]\n",
    "    print(f\"Batch size {batch_size}\")\n",
    "    device = input_doc.device\n",
    "\n",
    "    expanded_batch_idxs = (\n",
    "        torch.arange(batch_size)\n",
    "            .view(-1, 1)\n",
    "            .repeat(1, 1)\n",
    "            .view(-1)\n",
    "            .to(device)\n",
    "    )\n",
    "    encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "        0, expanded_batch_idxs\n",
    "    )\n",
    "\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                    \"past_key_values\": past_key_values,\n",
    "                    \"attention_mask\": attn_mask,\n",
    "                    \"encoder_outputs\": encoder_outputs,\n",
    "                    \"decoder_input_ids\": decoder_input_ids,\n",
    "                    }\n",
    "\n",
    "    outputs = model(**model_inputs, use_cache=False, return_dict=True)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    if attr_mode:\n",
    "        return next_token_logits.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101])\n",
      "torch.Size([1, 7])\n",
      "Batch size 1\n",
      "!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-!-\n",
      "torch.Size([1, 101])\n",
      "torch.Size([1, 7])\n",
      "Batch size 1\n",
      "torch.Size([1, 101])\n",
      "torch.Size([1, 7])\n",
      "Batch size 1\n",
      "torch.Size([50, 101])\n",
      "torch.Size([50, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (101) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-50-891299ae07a2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     35\u001B[0m attributions_ig, delta = lig.attribute((input_ids,decoder_input_ids), additional_forward_args=additional_input,\n\u001B[1;32m     36\u001B[0m                                        \u001B[0mbaselines\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreference_indices\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdec_reference_indices\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m                                            n_steps=50, return_convergence_delta=True)\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001B[0m in \u001B[0;36mattribute\u001B[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001B[0m\n\u001B[1;32m    358\u001B[0m             \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    359\u001B[0m             \u001B[0minternal_batch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minternal_batch_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 360\u001B[0;31m             \u001B[0mreturn_convergence_delta\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    361\u001B[0m         )\n\u001B[1;32m    362\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/captum/attr/_core/integrated_gradients.py\u001B[0m in \u001B[0;36mattribute\u001B[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001B[0m\n\u001B[1;32m    282\u001B[0m             \u001B[0minternal_batch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minternal_batch_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0mforward_fn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward_func\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m             \u001B[0mtarget_ind\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mexpanded_target\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m         )\n\u001B[1;32m    286\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/captum/attr/_utils/batching.py\u001B[0m in \u001B[0;36m_batched_operator\u001B[0;34m(operator, inputs, additional_forward_args, target_ind, internal_batch_size, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         )\n\u001B[1;32m    163\u001B[0m         for input, additional, target in _batched_generator(\n\u001B[0;32m--> 164\u001B[0;31m             \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madditional_forward_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_ind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minternal_batch_size\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    165\u001B[0m         )\n\u001B[1;32m    166\u001B[0m     ]\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/captum/attr/_utils/batching.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    161\u001B[0m             \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m         )\n\u001B[0;32m--> 163\u001B[0;31m         for input, additional, target in _batched_generator(\n\u001B[0m\u001B[1;32m    164\u001B[0m             \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madditional_forward_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_ind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minternal_batch_size\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m         )\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001B[0m in \u001B[0;36mgradient_func\u001B[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m                 output = _run_forward(\n\u001B[0;32m--> 334\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_ind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madditional_forward_args\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    335\u001B[0m                 )\n\u001B[1;32m    336\u001B[0m                 \u001B[0mhook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/captum/attr/_utils/common.py\u001B[0m in \u001B[0;36m_run_forward\u001B[0;34m(forward_func, inputs, target, additional_forward_args)\u001B[0m\n\u001B[1;32m    501\u001B[0m         \u001B[0;34m*\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0madditional_forward_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    502\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0madditional_forward_args\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 503\u001B[0;31m         \u001B[0;32melse\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    504\u001B[0m     )\n\u001B[1;32m    505\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0m_select_targets\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-49-eeff17bfac2d>\u001B[0m in \u001B[0;36mforward_step\u001B[0;34m(input_doc, decoder_input_ids, additional_input_args)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0;31m# input_doc = input_doc.permute(1,0)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;31m# decoder_input_ids = decoder_input_ids.permute(1,0)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0mencoder_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_doc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m     \u001B[0mbatch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput_doc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Batch size {batch_size}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    540\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 541\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    542\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/modeling_bart.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    333\u001B[0m         \u001B[0minputs_embeds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membed_tokens\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membed_scale\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m         \u001B[0membed_pos\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membed_positions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 335\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minputs_embeds\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0membed_pos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    336\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayernorm_embedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    337\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (7) must match the size of tensor b (101) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "\n",
    "token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)\n",
    "\n",
    "lig = LayerIntegratedGradients(forward_step, model.model.shared)\n",
    "\n",
    "TXT =\"The Pegasus model was proposed in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu on Dec 18, 2019. According to the abstract, Pegasus’ pretraining task is intentionally similar to summarization: important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\"\n",
    "input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']\n",
    "\n",
    "seq_length = input_ids.shape[1]\n",
    "reference_indices = token_reference.generate_reference(seq_length, device='cpu').unsqueeze(0)\n",
    "reference_indices[:,0] = tokenizer.bos_token_id\n",
    "reference_indices[:,-1] = tokenizer.eos_token_id\n",
    "\n",
    "device = input_ids.device\n",
    "decoded = [tokenizer.encode(\"The model is proposed by\")]\n",
    "decoder_input_ids = torch.LongTensor(decoded).to(device)\n",
    "dec_len = len(decoded[0])\n",
    "dec_reference_indices = token_reference.generate_reference(dec_len, device='cpu').unsqueeze(0)\n",
    "dec_reference_indices[:,0] = tokenizer.bos_token_id\n",
    "\n",
    "# input_ids = input_ids.permute(1,0)\n",
    "# decoder_input_ids = decoder_input_ids.permute(1,0)\n",
    "# reference_indices = reference_indices.permute(1,0)\n",
    "\n",
    "additional_input = {\n",
    "                \"attn_mask\": None,\n",
    "                \"past_key_values\": None,\n",
    "                \"decoder_input_ids\": decoder_input_ids, \"attr_mode\": True\n",
    "            }\n",
    "forward_step(input_doc=input_ids,decoder_input_ids=decoder_input_ids,additional_input_args=additional_input)\n",
    "print(\"!-\"*20)\n",
    "attributions_ig, delta = lig.attribute((input_ids,decoder_input_ids), additional_forward_args=additional_input,\n",
    "                                       baselines=(reference_indices,dec_reference_indices), \\\n",
    "                                           n_steps=50, return_convergence_delta=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}