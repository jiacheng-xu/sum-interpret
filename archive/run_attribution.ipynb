{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking /Users/jcxu/.cache/huggingface/datasets/a0c796d5dde60382897534854829044dab3d8f2cb3dddbbf1b7db89d7f21a61a.03dc4f0007c770026514ad44f62dcc3d2bcbbc3cb7004bfa1b166aefa790a994.py for additional imports.\n",
      "Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/xsum/xsum.py at /Users/jcxu/.cache/huggingface/modules/datasets_modules/datasets/xsum\n",
      "Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/xsum/xsum.py at /Users/jcxu/.cache/huggingface/modules/datasets_modules/datasets/xsum/128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae\n",
      "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/xsum/xsum.py to /Users/jcxu/.cache/huggingface/modules/datasets_modules/datasets/xsum/128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae/xsum.py\n",
      "Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/xsum/dataset_infos.json to /Users/jcxu/.cache/huggingface/modules/datasets_modules/datasets/xsum/128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae/dataset_infos.json\n",
      "Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/xsum/xsum.py at /Users/jcxu/.cache/huggingface/modules/datasets_modules/datasets/xsum/128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae/xsum.json\n",
      "Using custom data configuration default\n",
      "Loading Dataset Infos from /Users/jcxu/.cache/huggingface/modules/datasets_modules/datasets/xsum/128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae\n",
      "Overwrite dataset info from restored data version.\n",
      "Loading Dataset info from /Users/jcxu/.cache/huggingface/datasets/xsum/default/1.1.0/128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae\n",
      "Reusing dataset xsum (/Users/jcxu/.cache/huggingface/datasets/xsum/default/1.1.0/128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae)\n",
      "Constructing Dataset for split validation, from /Users/jcxu/.cache/huggingface/datasets/xsum/default/1.1.0/128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Lock 140257389421008 acquired on /Users/jcxu/.cache/huggingface/datasets/a0c796d5dde60382897534854829044dab3d8f2cb3dddbbf1b7db89d7f21a61a.03dc4f0007c770026514ad44f62dcc3d2bcbbc3cb7004bfa1b166aefa790a994.py.lock\n",
      "INFO - Lock 140257389421008 released on /Users/jcxu/.cache/huggingface/datasets/a0c796d5dde60382897534854829044dab3d8f2cb3dddbbf1b7db89d7f21a61a.03dc4f0007c770026514ad44f62dcc3d2bcbbc3cb7004bfa1b166aefa790a994.py.lock\n",
      "INFO - Lock 140244101367120 acquired on /Users/jcxu/.cache/huggingface/datasets/_Users_jcxu_.cache_huggingface_datasets_xsum_default_1.1.0_128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae.lock\n",
      "INFO - Lock 140244101367120 released on /Users/jcxu/.cache/huggingface/datasets/_Users_jcxu_.cache_huggingface_datasets_xsum_default_1.1.0_128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae.lock\n",
      "INFO - Lock 140244101909864 acquired on /Users/jcxu/.cache/huggingface/datasets/_Users_jcxu_.cache_huggingface_datasets_xsum_default_1.1.0_128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae.lock\n",
      "INFO - Lock 140244101909864 released on /Users/jcxu/.cache/huggingface/datasets/_Users_jcxu_.cache_huggingface_datasets_xsum_default_1.1.0_128741c17b7a4c939dbf844a75a5e83deadd07deaf4b2eda2056ed8eebdb03ae.lock\n",
      "INFO - {'document': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None)}\n",
      "INFO - {'document': 'Burberry reported pre-tax profits of £166m for the year to March. A year ago it made a loss of £16.1m, hit by charges at its Spanish operations.In the past year it has opened 21 new stores and closed nine. It plans to open 20-30 stores this year worldwide.The group has also focused on promoting the Burberry brand online.Sales rose 7% to £1.28bn, with the company recording double-digit sales growth in Europe and Asia Pacific.Adjusted profit rose 23% to £215m, taking into account one-off items and a favourable exchange rate.Stores in London in particular benefited from favourable currency movements and increased tourism.\"Looking forward, while mindful of the economic environment, Burberry plans to build on its strong financial position by accelerating investment in growth initiatives in retail, digital and new markets, while continuing to enhance the brand,\" said chief executive Angela Ahrendts.Burberry shares were up 7.6% at 659 pence in afternoon trading.\\n', 'summary': 'Luxury fashion designer Burberry has returned to profit after opening new stores and spending more on online marketing\\n'}\n",
      "INFO - ========================================\n",
      "INFO - loading archive file https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.03.24.tar.gz from cache at /Users/jcxu/.allennlp/cache/e20d5b792a8d456a1a61da245d1856d4b7778efe69ac3c30759af61940aa0f42.f72523a9682cb1f5ad3ecf834075fe53a1c25a6bcbf4b40c11e13b7f426a4724\n",
      "INFO - extracting archive file /Users/jcxu/.allennlp/cache/e20d5b792a8d456a1a61da245d1856d4b7778efe69ac3c30759af61940aa0f42.f72523a9682cb1f5ad3ecf834075fe53a1c25a6bcbf4b40c11e13b7f426a4724 to temp dir /var/folders/zk/bm40cl1n75j_cndxw0zz3kp40000gp/T/tmptl9z6k8f\n",
      "INFO - instantiating registered subclass srl_bert of <class 'allennlp.models.model.Model'>\n",
      "INFO - type = default\n",
      "INFO - instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "INFO - Loading token dictionary from /var/folders/zk/bm40cl1n75j_cndxw0zz3kp40000gp/T/tmptl9z6k8f/vocabulary.\n",
      "INFO - instantiating class <class 'allennlp.models.model.Model'> from params {'bert_model': 'bert-base-uncased', 'embedding_dropout': 0.1, 'type': 'srl_bert'} and extras {'vocab'}\n",
      "INFO - model.type = srl_bert\n",
      "INFO - instantiating class <class 'allennlp.models.srl_bert.SrlBert'> from params {'bert_model': 'bert-base-uncased', 'embedding_dropout': 0.1} and extras {'vocab'}\n",
      "INFO - model.bert_model = bert-base-uncased\n",
      "INFO - model.embedding_dropout = 0.1\n",
      "INFO - model.label_smoothing = None\n",
      "INFO - model.ignore_span_metric = False\n",
      "INFO - model.srl_eval_path = /Users/jcxu/anaconda3/lib/python3.6/site-packages/allennlp/tools/srl-eval.pl\n",
      "INFO - loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/jcxu/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO - extracting archive file /Users/jcxu/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/zk/bm40cl1n75j_cndxw0zz3kp40000gp/T/tmpqdmiqqtm\n",
      "INFO - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO - Initializing parameters\n",
      "INFO - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "INFO -    bert_model.embeddings.LayerNorm.bias\n",
      "INFO -    bert_model.embeddings.LayerNorm.weight\n",
      "INFO -    bert_model.embeddings.position_embeddings.weight\n",
      "INFO -    bert_model.embeddings.token_type_embeddings.weight\n",
      "INFO -    bert_model.embeddings.word_embeddings.weight\n",
      "INFO -    bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.0.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.0.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.0.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.0.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.0.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.0.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.0.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.0.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.1.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.1.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.1.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.1.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.1.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.1.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.1.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.1.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.10.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.10.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.10.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.10.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.10.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.10.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.10.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.10.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.11.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.11.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.11.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.11.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.11.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.11.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.11.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.11.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.2.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.2.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.2.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.2.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.2.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.2.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.2.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.2.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.3.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.3.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.3.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.3.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.3.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.3.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.3.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.3.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.4.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.4.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.4.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.4.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.4.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.4.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.4.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.4.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.5.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.5.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.5.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.5.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.5.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.5.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.5.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.5.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.6.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.6.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.6.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.6.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.6.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.6.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.6.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.6.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.7.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.7.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.7.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.7.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.7.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.7.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.7.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.7.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.8.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.8.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.8.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.8.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.8.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.8.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.8.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.8.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "INFO -    bert_model.encoder.layer.9.attention.self.key.bias\n",
      "INFO -    bert_model.encoder.layer.9.attention.self.key.weight\n",
      "INFO -    bert_model.encoder.layer.9.attention.self.query.bias\n",
      "INFO -    bert_model.encoder.layer.9.attention.self.query.weight\n",
      "INFO -    bert_model.encoder.layer.9.attention.self.value.bias\n",
      "INFO -    bert_model.encoder.layer.9.attention.self.value.weight\n",
      "INFO -    bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "INFO -    bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "INFO -    bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "INFO -    bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "INFO -    bert_model.encoder.layer.9.output.dense.bias\n",
      "INFO -    bert_model.encoder.layer.9.output.dense.weight\n",
      "INFO -    bert_model.pooler.dense.bias\n",
      "INFO -    bert_model.pooler.dense.weight\n",
      "INFO -    tag_projection_layer.bias\n",
      "INFO -    tag_projection_layer.weight\n",
      "INFO - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'bert_model_name': 'bert-base-uncased', 'type': 'srl'} and extras set()\n",
      "INFO - dataset_reader.type = srl\n",
      "INFO - instantiating class <class 'allennlp.data.dataset_readers.semantic_role_labeling.SrlReader'> from params {'bert_model_name': 'bert-base-uncased'} and extras set()\n",
      "INFO - dataset_reader.token_indexers = <allennlp.common.params.Params object at 0x7f8d86b2a160>\n",
      "INFO - dataset_reader.domain_identifier = None\n",
      "INFO - dataset_reader.lazy = False\n",
      "INFO - dataset_reader.bert_model_name = bert-base-uncased\n",
      "INFO - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/jcxu/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO - instantiating registered subclass semantic-role-labeling of <class 'allennlp.predictors.predictor.Predictor'>\n"
     ]
    }
   ],
   "source": [
    "from lib.attr_ig import summarize_attributions\n",
    "from lib.run_lime import get_xsum_data, init_model, tokenize_text\n",
    "import torch\n",
    "all_data = get_xsum_data()\n",
    "device = 'cuda:0'\n",
    "device = 'cpu'\n",
    "device = torch.device(device)\n",
    "mname = 'sshleifer/distilbart-cnn-6-6'\n",
    "model, tokenizer = init_model(mname=mname, device=device)\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.03.24.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "data = all_data[0]\n",
    "document, ref_sum = data['document'], data['summary']\n",
    "token_ids, doc_str,_ = tokenize_text(tokenizer, document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "def forward_enc_dec_step(model, encoder_outputs, decoder_input_ids):\n",
    "    # expanded_batch_idxs = (\n",
    "    #         torch.arange(batch_size)\n",
    "    #             .view(-1, 1)\n",
    "    #             .repeat(1, 1)\n",
    "    #             .view(-1)\n",
    "    #             .to(device)\n",
    "    #     )\n",
    "    # encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "    #         0, expanded_batch_idxs\n",
    "    #     )\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                        \"past_key_values\": None,\n",
    "                        \"encoder_outputs\": encoder_outputs,\n",
    "                        \"decoder_input_ids\": decoder_input_ids,\n",
    "                        }\n",
    "    outputs = model(**model_inputs, use_cache=False, return_dict=True,output_attentions=True)\n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def forward_step(model,encoder_outputs,  past_key_values, decoder_input_ids, inp_attn_mask=None):\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                        \"past_key_values\": past_key_values,\n",
    "                        \"attention_mask\": inp_attn_mask,\n",
    "                        \"encoder_outputs\": encoder_outputs,\n",
    "                        \"decoder_input_ids\": decoder_input_ids,\n",
    "                        }\n",
    "    outputs = model(**model_inputs, use_cache=True, return_dict=True)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    pred_distribution = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "    numpy_pred_distb = pred_distribution.detach().numpy()\n",
    "    ent = entropy(numpy_pred_distb,axis=-1)\n",
    "    top5 = torch.topk(pred_distribution, 5, dim=-1, largest=True, sorted=True)\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "    next_token = next_token.unsqueeze(-1)\n",
    "    cur_decoded = next_token.tolist()\n",
    "\n",
    "    if \"past_key_values\" in outputs:\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "    decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "    return ent, top5, cur_decoded,pred_distribution, past_key_values, decoder_input_ids\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "input_doc = token_ids['input_ids']\n",
    "encoder_outputs = model.model.encoder(input_doc, return_dict=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "from captum.attr import TokenReferenceBase\n",
    "\n",
    "batch_size = input_doc.shape[0]\n",
    "cur_len = 1\n",
    "max_len = 6\n",
    "has_eos = [False for _ in range(batch_size)]\n",
    "bos_token_id = tokenizer.bos_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "decoded = [[bos_token_id] for _ in range(batch_size)]\n",
    "decoder_input_ids = torch.LongTensor(decoded).to(device)\n",
    "past_key_values = None\n",
    "seq_length = input_doc.shape[1]\n",
    "token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)\n",
    "reference_indice = token_reference.generate_reference(seq_length, device=device)\n",
    "reference_indices = torch.stack([reference_indice for _ in range(batch_size)], dim=0)\n",
    "# reference_indices[:, 0] = self.tokenizer.bos_token_id\n",
    "# reference_indices[:, -1] = self.tokenizer.eos_token_id\n",
    "ref_encoder_outputs = model.model.encoder(reference_indices, return_dict=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "from lib.attr_ig import summarize_attributions,simple_viz_attribution\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "def fast_ig_enc_dec(decoded_inputs,tgt_class:int, encoder_outputs,ref_encoder_outputs ,device,num_steps = 51,):\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    interp_step_vec = ( encoder_outputs.last_hidden_state - ref_encoder_outputs.last_hidden_state) / num_steps\n",
    "    ranges = torch.arange(1, num_steps + 1).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    repeated_raw = ranges * interp_step_vec\n",
    "    interp_last_hidden_state = repeated_raw + ref_encoder_outputs.last_hidden_state\n",
    "    interp_encoder_outputs = ref_encoder_outputs\n",
    "    interp_encoder_outputs.last_hidden_state =interp_last_hidden_state\n",
    "\n",
    "    # interp_decoded = [decoded_inputs for _ in range(num_steps)]\n",
    "    interp_decoded = decoded_inputs.repeat(num_steps,1)\n",
    "    interp_decoder_input_ids =torch.LongTensor(interp_decoded).to(device)\n",
    "    with torch.enable_grad():\n",
    "        interp_encoder_outputs.last_hidden_state.retain_grad()\n",
    "        interp_out = forward_enc_dec_step(model,interp_encoder_outputs,interp_decoder_input_ids)\n",
    "        logits = interp_out.logits[:,-1,:]\n",
    "        target = torch.ones(num_steps,dtype=torch.long) * tgt_class\n",
    "\n",
    "        loss = loss_fct(logits, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        logging.info(f\"Loss: {loss.tolist()}\")\n",
    "        raw_grad = interp_encoder_outputs.last_hidden_state.grad\n",
    "\n",
    "        # Approximate the integral using the trapezodal rule\n",
    "        approx_grad = (raw_grad[:-1] + raw_grad[1:]) / 2\n",
    "        # print(approx_grad.size())\n",
    "        avg_grad = torch.mean(approx_grad, dim=0)  # input len, hdim\n",
    "        # print(encoder_outputs.last_hidden_state.size())\n",
    "        # print(ref_encoder_outputs.last_hidden_state.size())\n",
    "        integrated_gradient = (encoder_outputs.last_hidden_state - ref_encoder_outputs.last_hidden_state[0]) * avg_grad  # seq_len, hdim\n",
    "    return integrated_gradient\n",
    "\"\"\"\n",
    "attr_mode = True\n",
    "num_steps = 51\n",
    "loss_fct = CrossEntropyLoss()\n",
    "interp_step_vec = ( encoder_outputs.last_hidden_state - ref_encoder_outputs.last_hidden_state) / num_steps\n",
    "ranges = torch.arange(1, num_steps + 1).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "repeated_raw = ranges * interp_step_vec\n",
    "interp_last_hidden_state = repeated_raw + ref_encoder_outputs.last_hidden_state\n",
    "interp_encoder_outputs = ref_encoder_outputs\n",
    "interp_encoder_outputs.last_hidden_state =interp_last_hidden_state\n",
    "interp_past_key_values=None\n",
    "interp_decoded = [[bos_token_id, bos_token_id,bos_token_id] for _ in range(num_steps)]\n",
    "interp_decoder_input_ids =torch.LongTensor(interp_decoded).to(device)\n",
    "with torch.enable_grad():\n",
    "    interp_encoder_outputs.last_hidden_state.retain_grad()\n",
    "    interp_out = forward_enc_dec_step(model,interp_encoder_outputs,interp_decoder_input_ids)\n",
    "    logits = interp_out.logits[:,-1,:]\n",
    "    target = torch.ones(num_steps,dtype=torch.long)\n",
    "\n",
    "    loss = loss_fct(logits, target)\n",
    "    loss.backward(retain_graph=True)\n",
    "    logging.info(f\"Loss: {loss.tolist()}\")\n",
    "    raw_grad = interp_encoder_outputs.last_hidden_state.grad\n",
    "\n",
    "    # Approximate the integral using the trapezodal rule\n",
    "    approx_grad = (raw_grad[:-1] + raw_grad[1:]) / 2\n",
    "    avg_grad = torch.mean(approx_grad, dim=0)  # input len, hdim\n",
    "    integrated_gradient = (encoder_outputs.last_hidden_state - ref_encoder_outputs.last_hidden_state) * avg_grad  # seq_len, hdim\n",
    "    print(integrated_gradient)\n",
    "    # extracted_attribution = summarize_attributions(integrated_gradient)\n",
    "    # viz = simple_viz_attribution(tokenizer, input_ids_tensor, extracted_attribution)\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "\n",
    "# do linear interpolation of these embeddings\n",
    "# interp_step_vec = (embed_input_tensor - embed_ref_tensor) / num_steps\n",
    "# ranges = torch.arange(1, num_steps + 1).unsqueeze(-1).unsqueeze(-1).to(self.model.device)\n",
    "# repeated_raw = ranges * interp_step_vec\n",
    "# interpolated_tensor = repeated_raw + embed_ref_tensor\n",
    "\n",
    "# run the model forward\n",
    "# with torch.enable_grad():\n",
    "#     interpolated_tensor.retain_grad()\n",
    "\n",
    "all_entropy = []\n",
    "all_topk = []\n",
    "while cur_len < max_len and (not all(has_eos)):\n",
    "    cur_len +=1\n",
    "    logging.debug(f\"Step: {cur_len}\")\n",
    "    last_decoder_input_ids =decoder_input_ids\n",
    "    ent, top5, cur_decoded, pred_distribution,past_key_values, decoder_input_ids = forward_step(model,encoder_outputs,past_key_values,decoder_input_ids)\n",
    "    all_entropy.append(ent[0])\n",
    "    all_topk.append(top5)\n",
    "    cur_decoded = [ cur_dec_token[0] for cur_dec_token in cur_decoded]\n",
    "    # print(cur_decoded)\n",
    "    for idx in range(batch_size):\n",
    "        if cur_decoded[idx] == tokenizer.eos_token_id or cur_decoded[idx] == 479:\n",
    "            has_eos[idx]=True\n",
    "    # print(cur_decoded[0])\n",
    "    # Attribution\n",
    "    # print(last_decoder_input_ids)\n",
    "    ig = fast_ig_enc_dec(decoded_inputs=last_decoder_input_ids,\n",
    "                         tgt_class=cur_decoded[0],\n",
    "                         encoder_outputs=encoder_outputs,\n",
    "                         ref_encoder_outputs=ref_encoder_outputs,device=device)\n",
    "    extracted_attribution = summarize_attributions(ig)\n",
    "\n",
    "    # process for viz\n",
    "    extracted_attribution = extracted_attribution.squeeze(0)\n",
    "    input_doc = input_doc.squeeze(0)\n",
    "    viz = simple_viz_attribution(tokenizer, input_doc, extracted_attribution)\n",
    "    logging.info(viz)\n",
    "\n",
    "decoder_input_ids = decoder_input_ids[:,1:] # REMOVE <s>\n",
    "all_decoded_tokens = decoder_input_ids.tolist()\n",
    "decoded_sents = [[ tokenizer.convert_ids_to_tokens(x) for x in s] for s in all_decoded_tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS\n",
      "SS\n",
      " ss\n",
      " as\n",
      " a\n",
      " :\n",
      "good\n",
      "'\n",
      "boy\n",
      " '\n",
      "but\n",
      "'\n",
      " not\n",
      " real\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 5, 6, 7, 8, 9]"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def align_bpe_and_words(word_tokens, bpe_tokens, tokenizer):\n",
    "    # word_tokens are list of str, bpe_tokens are list of int\n",
    "    cache_word = \"\"\n",
    "    cache_bpe = \"\"\n",
    "    map_from_bpe_to_word = [-1 for _ in range(len(bpe_tokens))]\n",
    "    map_cursor_bpe = 0\n",
    "    map_cursor_word = 0\n",
    "    word_tokens = [w.translate({ord(c): None for c in string.whitespace}) for w in word_tokens]\n",
    "    bpe_txts = [ tokenizer.decode(x).translate({ord(c): None for c in string.whitespace}) for x in bpe_tokens]\n",
    "    # print(word_tokens)\n",
    "    # print(bpe_txts)\n",
    "    while len(word_tokens) > 0 and len(bpe_txts)>0:\n",
    "        if len(cache_bpe) == len(cache_word):\n",
    "            _pop_bpe = bpe_txts.pop(0)\n",
    "            _pop_word = word_tokens.pop(0)\n",
    "            cache_bpe += _pop_bpe\n",
    "            cache_word += _pop_word\n",
    "            map_from_bpe_to_word[map_cursor_bpe] = map_cursor_word\n",
    "            map_cursor_bpe += 1\n",
    "            if len(cache_word) == len(cache_bpe):\n",
    "               map_cursor_word+=1\n",
    "        elif len(cache_word)>len(cache_bpe):\n",
    "            _pop_bpe = bpe_txts.pop(0)\n",
    "            cache_bpe += _pop_bpe\n",
    "            map_from_bpe_to_word[map_cursor_bpe] = map_cursor_word\n",
    "            map_cursor_bpe += 1\n",
    "        else:\n",
    "            raise RuntimeError(f\"{bpe_txts}\\n{word_tokens}\")\n",
    "    # print(map_from_bpe_to_word)\n",
    "    # print(word_tokens)\n",
    "    # print(bpe_txts)\n",
    "    return map_from_bpe_to_word\n",
    "\n",
    "src = \"ASSS ss as a :good'boy 'but' not real.\"\n",
    "# src = \"The SRL model was evaluated on the CoNLL 2012 dataset. \"\n",
    "srl_pred = predictor.predict(sentence=src)\n",
    "srl_words = srl_pred['words']\n",
    "codes = tokenizer.encode(src)\n",
    "codes = codes[1:-1]\n",
    "for c in codes:\n",
    "    print(tokenizer.decode(c))\n",
    "txts = tokenizer.convert_ids_to_tokens(codes)\n",
    "align_bpe_and_words(srl_words,codes,tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}