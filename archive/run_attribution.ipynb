{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9cddb64825fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr_ig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize_attributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_lime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_xsum_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_xsum_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "from lib.attr_ig import summarize_attributions\n",
    "from lib.run_lime import get_xsum_data, init_model, tokenize_text\n",
    "import torch\n",
    "all_data = get_xsum_data()\n",
    "device = 'cuda:0'\n",
    "device = 'cpu'\n",
    "device = torch.device(device)\n",
    "mname = 'sshleifer/distilbart-cnn-6-6'\n",
    "model, tokenizer = init_model(mname=mname, device=device)\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.03.24.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-97250d52d090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "data = all_data[0]\n",
    "document, ref_sum = data['document'], data['summary']\n",
    "token_ids, doc_str,_ = tokenize_text(tokenizer, document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def forward_enc_dec_step(model, encoder_outputs, decoder_input_ids):\n",
    "    # expanded_batch_idxs = (\n",
    "    #         torch.arange(batch_size)\n",
    "    #             .view(-1, 1)\n",
    "    #             .repeat(1, 1)\n",
    "    #             .view(-1)\n",
    "    #             .to(device)\n",
    "    #     )\n",
    "    # encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n",
    "    #         0, expanded_batch_idxs\n",
    "    #     )\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                        \"past_key_values\": None,\n",
    "                        \"encoder_outputs\": encoder_outputs,\n",
    "                        \"decoder_input_ids\": decoder_input_ids,\n",
    "                        }\n",
    "    outputs = model(**model_inputs, use_cache=False, return_dict=True,output_attentions=True)\n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def forward_step(model,encoder_outputs,  past_key_values, decoder_input_ids, inp_attn_mask=None):\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                        \"past_key_values\": past_key_values,\n",
    "                        \"attention_mask\": inp_attn_mask,\n",
    "                        \"encoder_outputs\": encoder_outputs,\n",
    "                        \"decoder_input_ids\": decoder_input_ids,\n",
    "                        }\n",
    "    outputs = model(**model_inputs, use_cache=True, return_dict=True)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    pred_distribution = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "    numpy_pred_distb = pred_distribution.detach().numpy()\n",
    "    ent = entropy(numpy_pred_distb,axis=-1)\n",
    "    top5 = torch.topk(pred_distribution, 5, dim=-1, largest=True, sorted=True)\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "    next_token = next_token.unsqueeze(-1)\n",
    "    cur_decoded = next_token.tolist()\n",
    "\n",
    "    if \"past_key_values\" in outputs:\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "    decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "    return ent, top5, cur_decoded,pred_distribution, past_key_values, decoder_input_ids\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'token_ids' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2340a3dbfa7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token_ids' is not defined"
     ]
    }
   ],
   "source": [
    "input_doc = token_ids['input_ids']\n",
    "encoder_outputs = model.model.encoder(input_doc, return_dict=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from captum.attr import TokenReferenceBase\n",
    "\n",
    "batch_size = input_doc.shape[0]\n",
    "cur_len = 1\n",
    "max_len = 6\n",
    "has_eos = [False for _ in range(batch_size)]\n",
    "bos_token_id = tokenizer.bos_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "decoded = [[bos_token_id] for _ in range(batch_size)]\n",
    "decoder_input_ids = torch.LongTensor(decoded).to(device)\n",
    "past_key_values = None\n",
    "seq_length = input_doc.shape[1]\n",
    "token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)\n",
    "reference_indice = token_reference.generate_reference(seq_length, device=device)\n",
    "reference_indices = torch.stack([reference_indice for _ in range(batch_size)], dim=0)\n",
    "# reference_indices[:, 0] = self.tokenizer.bos_token_id\n",
    "# reference_indices[:, -1] = self.tokenizer.eos_token_id\n",
    "ref_encoder_outputs = model.model.encoder(reference_indices, return_dict=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lib.attr_ig import summarize_attributions,simple_viz_attribution\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "def fast_ig_enc_dec(decoded_inputs,tgt_class:int, encoder_outputs,ref_encoder_outputs ,device,num_steps = 51,):\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    interp_step_vec = ( encoder_outputs.last_hidden_state - ref_encoder_outputs.last_hidden_state) / num_steps\n",
    "    ranges = torch.arange(1, num_steps + 1).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    repeated_raw = ranges * interp_step_vec\n",
    "    interp_last_hidden_state = repeated_raw + ref_encoder_outputs.last_hidden_state\n",
    "    interp_encoder_outputs = ref_encoder_outputs\n",
    "    interp_encoder_outputs.last_hidden_state =interp_last_hidden_state\n",
    "\n",
    "    # interp_decoded = [decoded_inputs for _ in range(num_steps)]\n",
    "    interp_decoded = decoded_inputs.repeat(num_steps,1)\n",
    "    interp_decoder_input_ids =torch.LongTensor(interp_decoded).to(device)\n",
    "    with torch.enable_grad():\n",
    "        interp_encoder_outputs.last_hidden_state.retain_grad()\n",
    "        interp_out = forward_enc_dec_step(model,interp_encoder_outputs,interp_decoder_input_ids)\n",
    "        logits = interp_out.logits[:,-1,:]\n",
    "        target = torch.ones(num_steps,dtype=torch.long) * tgt_class\n",
    "\n",
    "        loss = loss_fct(logits, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        logging.info(f\"Loss: {loss.tolist()}\")\n",
    "        raw_grad = interp_encoder_outputs.last_hidden_state.grad\n",
    "\n",
    "        # Approximate the integral using the trapezodal rule\n",
    "        approx_grad = (raw_grad[:-1] + raw_grad[1:]) / 2\n",
    "        # print(approx_grad.size())\n",
    "        avg_grad = torch.mean(approx_grad, dim=0)  # input len, hdim\n",
    "        # print(encoder_outputs.last_hidden_state.size())\n",
    "        # print(ref_encoder_outputs.last_hidden_state.size())\n",
    "        integrated_gradient = (encoder_outputs.last_hidden_state - ref_encoder_outputs.last_hidden_state[0]) * avg_grad  # seq_len, hdim\n",
    "    return integrated_gradient\n",
    "\"\"\"\n",
    "attr_mode = True\n",
    "num_steps = 51\n",
    "loss_fct = CrossEntropyLoss()\n",
    "interp_step_vec = ( encoder_outputs.last_hidden_state - ref_encoder_outputs.last_hidden_state) / num_steps\n",
    "ranges = torch.arange(1, num_steps + 1).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "repeated_raw = ranges * interp_step_vec\n",
    "interp_last_hidden_state = repeated_raw + ref_encoder_outputs.last_hidden_state\n",
    "interp_encoder_outputs = ref_encoder_outputs\n",
    "interp_encoder_outputs.last_hidden_state =interp_last_hidden_state\n",
    "interp_past_key_values=None\n",
    "interp_decoded = [[bos_token_id, bos_token_id,bos_token_id] for _ in range(num_steps)]\n",
    "interp_decoder_input_ids =torch.LongTensor(interp_decoded).to(device)\n",
    "with torch.enable_grad():\n",
    "    interp_encoder_outputs.last_hidden_state.retain_grad()\n",
    "    interp_out = forward_enc_dec_step(model,interp_encoder_outputs,interp_decoder_input_ids)\n",
    "    logits = interp_out.logits[:,-1,:]\n",
    "    target = torch.ones(num_steps,dtype=torch.long)\n",
    "\n",
    "    loss = loss_fct(logits, target)\n",
    "    loss.backward(retain_graph=True)\n",
    "    logging.info(f\"Loss: {loss.tolist()}\")\n",
    "    raw_grad = interp_encoder_outputs.last_hidden_state.grad\n",
    "\n",
    "    # Approximate the integral using the trapezodal rule\n",
    "    approx_grad = (raw_grad[:-1] + raw_grad[1:]) / 2\n",
    "    avg_grad = torch.mean(approx_grad, dim=0)  # input len, hdim\n",
    "    integrated_gradient = (encoder_outputs.last_hidden_state - ref_encoder_outputs.last_hidden_state) * avg_grad  # seq_len, hdim\n",
    "    print(integrated_gradient)\n",
    "    # extracted_attribution = summarize_attributions(integrated_gradient)\n",
    "    # viz = simple_viz_attribution(tokenizer, input_ids_tensor, extracted_attribution)\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# do linear interpolation of these embeddings\n",
    "# interp_step_vec = (embed_input_tensor - embed_ref_tensor) / num_steps\n",
    "# ranges = torch.arange(1, num_steps + 1).unsqueeze(-1).unsqueeze(-1).to(self.model.device)\n",
    "# repeated_raw = ranges * interp_step_vec\n",
    "# interpolated_tensor = repeated_raw + embed_ref_tensor\n",
    "\n",
    "# run the model forward\n",
    "# with torch.enable_grad():\n",
    "#     interpolated_tensor.retain_grad()\n",
    "\n",
    "all_entropy = []\n",
    "all_topk = []\n",
    "while cur_len < max_len and (not all(has_eos)):\n",
    "    cur_len +=1\n",
    "    logging.debug(f\"Step: {cur_len}\")\n",
    "    last_decoder_input_ids =decoder_input_ids\n",
    "    ent, top5, cur_decoded, pred_distribution,past_key_values, decoder_input_ids = forward_step(model,encoder_outputs,past_key_values,decoder_input_ids)\n",
    "    all_entropy.append(ent[0])\n",
    "    all_topk.append(top5)\n",
    "    cur_decoded = [ cur_dec_token[0] for cur_dec_token in cur_decoded]\n",
    "    # print(cur_decoded)\n",
    "    for idx in range(batch_size):\n",
    "        if cur_decoded[idx] == tokenizer.eos_token_id or cur_decoded[idx] == 479:\n",
    "            has_eos[idx]=True\n",
    "    # print(cur_decoded[0])\n",
    "    # Attribution\n",
    "    # print(last_decoder_input_ids)\n",
    "    ig = fast_ig_enc_dec(decoded_inputs=last_decoder_input_ids,\n",
    "                         tgt_class=cur_decoded[0],\n",
    "                         encoder_outputs=encoder_outputs,\n",
    "                         ref_encoder_outputs=ref_encoder_outputs,device=device)\n",
    "    extracted_attribution = summarize_attributions(ig)\n",
    "\n",
    "    # process for viz\n",
    "    extracted_attribution = extracted_attribution.squeeze(0)\n",
    "    input_doc = input_doc.squeeze(0)\n",
    "    viz = simple_viz_attribution(tokenizer, input_doc, extracted_attribution)\n",
    "    logging.info(viz)\n",
    "\n",
    "decoder_input_ids = decoder_input_ids[:,1:] # REMOVE <s>\n",
    "all_decoded_tokens = decoder_input_ids.tolist()\n",
    "decoded_sents = [[ tokenizer.convert_ids_to_tokens(x) for x in s] for s in all_decoded_tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import string\n",
    "def align_bpe_and_words(word_tokens, bpe_tokens, tokenizer):\n",
    "    # word_tokens are list of str, bpe_tokens are list of int\n",
    "    cache_word = \"\"\n",
    "    cache_bpe = \"\"\n",
    "    map_from_bpe_to_word = [-1 for _ in range(len(bpe_tokens))]\n",
    "    map_cursor_bpe = 0\n",
    "    map_cursor_word = 0\n",
    "    word_tokens = [w.translate({ord(c): None for c in string.whitespace}) for w in word_tokens]\n",
    "    bpe_txts = [ tokenizer.decode(x).translate({ord(c): None for c in string.whitespace}) for x in bpe_tokens]\n",
    "    # print(word_tokens)\n",
    "    # print(bpe_txts)\n",
    "    while len(word_tokens) > 0 and len(bpe_txts)>0:\n",
    "        if len(cache_bpe) == len(cache_word):\n",
    "            _pop_bpe = bpe_txts.pop(0)\n",
    "            _pop_word = word_tokens.pop(0)\n",
    "            cache_bpe += _pop_bpe\n",
    "            cache_word += _pop_word\n",
    "            map_from_bpe_to_word[map_cursor_bpe] = map_cursor_word\n",
    "            map_cursor_bpe += 1\n",
    "            if len(cache_word) == len(cache_bpe):\n",
    "               map_cursor_word+=1\n",
    "        elif len(cache_word)>len(cache_bpe):\n",
    "            _pop_bpe = bpe_txts.pop(0)\n",
    "            cache_bpe += _pop_bpe\n",
    "            map_from_bpe_to_word[map_cursor_bpe] = map_cursor_word\n",
    "            map_cursor_bpe += 1\n",
    "        else:\n",
    "            raise RuntimeError(f\"{bpe_txts}\\n{word_tokens}\")\n",
    "    # print(map_from_bpe_to_word)\n",
    "    # print(word_tokens)\n",
    "    # print(bpe_txts)\n",
    "    return map_from_bpe_to_word\n",
    "\n",
    "src = \"ASSS ss as a :good'boy 'but' not real.\"\n",
    "# src = \"The SRL model was evaluated on the CoNLL 2012 dataset. \"\n",
    "srl_pred = predictor.predict(sentence=src)\n",
    "srl_words = srl_pred['words']\n",
    "codes = tokenizer.encode(src)\n",
    "codes = codes[1:-1]\n",
    "for c in codes:\n",
    "    print(tokenizer.decode(c))\n",
    "txts = tokenizer.convert_ids_to_tokens(codes)\n",
    "align_bpe_and_words(srl_words,codes,tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}