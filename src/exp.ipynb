{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "# see ``examples/summarization/bart/run_eval.py`` for a longer example\n",
    "bart_cnn = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_xsum = BartForConditionalGeneration.from_pretrained('facebook/bart-large-xsum')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "lm_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", force_bos_token_to_be_generated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_lm(model, tokenizer, device, sum_prefix=\"\", topk=10):\n",
    "    sum_prefix = sum_prefix.strip()\n",
    "    # Mask filling only works for bart-large\n",
    "    TXT = f\"{sum_prefix}<mask> \"    # we basically remove all of the last step cases.\n",
    "    input_ids = tokenizer([TXT], return_tensors='pt')['input_ids'].to(device)\n",
    "    logits = model(input_ids, return_dict=True)['logits']\n",
    "    masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "    probs = logits[0, masked_index].softmax(dim=0)\n",
    "    values, predictions = probs.topk(topk)\n",
    "    top_output = tokenizer.decode(predictions).split()\n",
    "    return top_output[0], probs\n",
    "\n",
    "\n",
    "def run_full_model(model, tokenizer, input_text, sum_prefix, encoder_outputs=None, device='cuda:0', output_attentions=False, output_dec_hid=False):\n",
    "\n",
    "    if not encoder_outputs:\n",
    "        inputs = tokenizer(input_text, max_length=300,\n",
    "                           return_tensors='pt', truncation=True, padding=True)\n",
    "        encoder_outputs = model.model.encoder(\n",
    "            inputs['input_ids'].to(device), return_dict=True)\n",
    "\n",
    "    sum_prefix = sum_prefix.strip()\n",
    "    batch_size = len(input_text)\n",
    "    decoder_input_ids = torch.LongTensor(tokenizer.encode(\n",
    "        sum_prefix, return_tensors='pt')).to(device)\n",
    "    decoder_input_ids = decoder_input_ids.expand(\n",
    "        (batch_size, decoder_input_ids.size()[-1]))\n",
    "    # ATTN: remove the EOS token from the prefix!\n",
    "    decoder_input_ids = decoder_input_ids[:, :-1]\n",
    "\n",
    "    model.output_attentions = output_attentions\n",
    "    model_inputs = {\"input_ids\": None,\n",
    "                    \"past_key_values\": None,\n",
    "                    \"encoder_outputs\": encoder_outputs,\n",
    "                    \"decoder_input_ids\": decoder_input_ids,\n",
    "                    }\n",
    "    outputs = model(**model_inputs, output_attentions=output_attentions, output_hidden_states=output_dec_hid,\n",
    "                    use_cache=False, return_dict=True)\n",
    "\n",
    "    if output_attentions:\n",
    "        # use cross attention as the distribution\n",
    "        # last layer.   batch=1, head, dec len, enc len\n",
    "        # by default we use the last layer of attention\n",
    "        cross_attns = outputs['cross_attentions'][-1]\n",
    "        attn = cross_attns[0, :, -1, :]    # head, enc len\n",
    "\n",
    "        mean_attn = torch.mean(attn, dim=0)\n",
    "        assert len(mean_attn.size()) == 1\n",
    "\n",
    "        topk = min(30, mean_attn.size()[0])\n",
    "\n",
    "        values, indices = torch.topk(mean_attn, k=topk)\n",
    "        values = values.detach().cpu().tolist()\n",
    "        indices = indices.detach().cpu().tolist()\n",
    "        input_ids_list = inputs['input_ids'][0].tolist()  # batch=1, enc len\n",
    "        output = tokenizer.decode(\n",
    "            int(input_ids_list[indices[0]]))\n",
    "        logging.info(f\"Most attention: {output}\")\n",
    "        p_list = [0.0 for _ in range(tokenizer.vocab_size)]\n",
    "        for v, i in zip(values, indices):\n",
    "            p_list[input_ids_list[i]] += v\n",
    "        p = torch.as_tensor(p_list, device=device)\n",
    "        p = p / p.sum()\n",
    "        return output, p\n",
    "    else:\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        prob = next_token_logits.softmax(dim=-1)\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "        # next_token = next_token.unsqueeze(-1)\n",
    "        next_token = next_token.tolist()\n",
    "\n",
    "        output = [tokenizer.decode(tk) for tk in next_token]\n",
    "        logging.info(f\"Next token: {output}\")\n",
    "        outputs['output'] = output\n",
    "        return outputs, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')\n",
    "import logging\n",
    "logger = logging.getLogger('sum')\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "def pnum(num):\n",
    "    return \"{:.2f}\".format(num)\n",
    "\n",
    "def show_top_k(prob, prefix, name, tokenizer, k=5):\n",
    "    prob = prob.squeeze()\n",
    "    topk_v, top_idx = torch.topk(prob, k=k)\n",
    "    index = top_idx.tolist()\n",
    "    toks = [tokenizer.decode(i) for i in index]\n",
    "    print(f\"Type: {name}\")\n",
    "    for i, t in enumerate(toks):\n",
    "        print(f\"{i}: {pnum(topk_v[i].item())} {prefix}{t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Bri\"\n",
    "# input=\"Brisk is a tea and juice brand managed by the Pepsi Lipton Partnership.\"\n",
    "input=\"Whether you’re only just thinking about getting engaged (or hinting at it and looking for the perfect ring), in the throes of wedding planning, or already navigating newlywed life, Brides is here to inspire, guide, and entertain you during this exciting, and trying, time. Looking for the most gorgeous wedding dress? \"\n",
    "# input=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-2-99a4d9b1a39c>:7: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
      "Type: lm\n",
      "0: 0.02 Bri B\n",
      "1: 0.01 Bri R\n",
      "2: 0.01 Bri H\n",
      "3: 0.01 Bri L\n",
      "4: 0.01 Bri D\n",
      "Type: xsum\n",
      "0: 0.35 Brianna\n",
      "1: 0.13 Briana\n",
      "2: 0.02 Bribery\n",
      "3: 0.02 Briar\n",
      "4: 0.02 Briley\n",
      "Type: cnn\n",
      "0: 0.23 Bripe\n",
      "1: 0.15 Brips\n",
      "2: 0.09 Bri.\n",
      "3: 0.04 Bripping\n",
      "4: 0.03 Brivers\n"
     ]
    }
   ],
   "source": [
    "_, lm_prob = run_lm(lm_model,tokenizer,device, prefix)\n",
    "show_top_k(lm_prob,prefix,'lm',tokenizer)\n",
    "\n",
    "_, xsum_prob = run_full_model(bart_xsum, tokenizer,[input],prefix,device=device)\n",
    "show_top_k(xsum_prob,prefix,'xsum',tokenizer)\n",
    "\n",
    "\n",
    "_, cnn_prob = run_full_model(bart_cnn, tokenizer,[input],prefix,device=device)\n",
    "show_top_k(cnn_prob,prefix,'cnn',tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\n[ 0. 10. 11.  0.  0.]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.,  0., 21.,  0.,  0.])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "\n",
    "x = np.zeros(5)\n",
    "x[1] = 10\n",
    "x[2] = 11\n",
    "\n",
    "eye = np.eye(5)\n",
    "eye[1][1] = 0\n",
    "eye[1][2] = 1\n",
    "\n",
    "\n",
    "print(eye)\n",
    "print(x)\n",
    "np.matmul(x,eye)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer.convert_tokens_to_ids(['x',' x','x'])\n",
    "# define a new matrix to get the mapping of all \"_x\" to \"x\"\n",
    "\n",
    "def init_vocab_distb_fix(tokenizer)->torch.Tensor:\n",
    "    trans_mat = np.eye(tokenizer.vocab_size)\n",
    "    cnt=0\n",
    "    for vocab_idx in range(tokenizer.vocab_size):\n",
    "        tok = tokenizer.convert_ids_to_tokens(vocab_idx)\n",
    "        if tok.startswith('Ġ'):\n",
    "            no_space_tok = tok[1:]\n",
    "            no_space_id = tokenizer.convert_tokens_to_ids(no_space_tok)\n",
    "            if no_space_id == 3:\n",
    "                continue\n",
    "            logging.debug(f\"{vocab_idx}:{tok} -> {no_space_id}:{tokenizer.convert_ids_to_tokens(no_space_id)}\")\n",
    "            trans_mat[vocab_idx][vocab_idx] = 0\n",
    "            trans_mat[vocab_idx][no_space_id] = 1\n",
    "            cnt+=1\n",
    "    logging.info(f\"Lines of change: {cnt}\")\n",
    "    return torch.from_numpy(trans_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}